# [] Importa√ß√µes necess√°rias
import os
from dotenv import load_dotenv
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferWindowMemory
from langchain.vectorstores import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
import google.generativeai as genai
import warnings

load_dotenv()  
warnings.filterwarnings('ignore')

# Cria√ß√£o de documentos de exemplo sobre IA e Machine Learning
documentos_exemplo = [
    """Intelig√™ncia Artificial (IA) √© um campo da ci√™ncia da computa√ß√£o que se concentra
na cria√ß√£o de sistemas capazes de realizar tarefas que normalmente requerem intelig√™ncia humana.
Isso inclui aprendizado, racioc√≠nio, percep√ß√£o e tomada de decis√µes.""",

    """Machine Learning √© uma sub√°rea da IA que permite que computadores aprendam e melhorem
automaticamente atrav√©s da experi√™ncia, sem serem explicitamente programados.
Os algoritmos de ML identificam padr√µes em dados e fazem previs√µes.""",

    """Deep Learning √© uma t√©cnica de machine learning baseada em redes neurais artificiais
com m√∫ltiplas camadas. √â especialmente eficaz para tarefas como reconhecimento de imagem,
processamento de linguagem natural e reconhecimento de voz.""",

    """RAG (Retrieval-Augmented Generation) √© uma t√©cnica que combina recupera√ß√£o de informa√ß√µes
com gera√ß√£o de texto. Permite que modelos de linguagem acessem conhecimento externo
para gerar respostas mais precisas e atualizadas.""",

    """LangChain √© um framework para desenvolvimento de aplica√ß√µes com modelos de linguagem.
Facilita a cria√ß√£o de cadeias complexas, gerenciamento de mem√≥ria e integra√ß√£o
com diferentes fontes de dados.""",
    
    """Google Gemini √© um modelo de linguagem multimodal desenvolvido pelo Google,
capaz de processar texto, imagens e c√≥digo. Oferece capacidades avan√ßadas de
racioc√≠nio e compreens√£o contextual."""
]

# Convers√£o para objetos Document
docs = [Document(page_content=doc) for doc in documentos_exemplo]
print(f"‚úÖ Criados {len(docs)} documentos de exemplo")

embeddings = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001",
    google_api_key= load_dotenv()['GOOGLE_API_KEY']

)

# Criacao do vector store
vectorstore = Chroma.from_documents(
    documents=docs,
    embedding=embeddings,
    persist_directory="./chroma_db_gemini"
)

print(f"N√∫mero de documentos indexados: {vectorstore._collection.count()}")

memory = ConversationBufferWindowMemory(
    k=5,
    memory_key="chat_history",
    return_messages=True
)

memory = ConversationBufferWindowMemory(
    k=5,
    memory_key="chat_history",
    return_messages=True,
    output_key="answer"
)

print("‚úÖ Mem√≥ria configurada!")
print(memory.k)

# Inicializa√ß√£o do modelo Google Gemini
llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-pro-latest",
    google_api_key=load_dotenv()['GOOGLE_API_KEY'],
    temperature=0.7,
    convert_system_message_to_human=True
)

qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
    memory=memory,
    return_source_documents=True,
    verbose=True
)

def fazer_pergunta(pergunta):
    """Fun√ß√£o auxiliar para fazer perguntas √† cadeia conversacional"""
    print(f"\n Pergunta: {pergunta}")
    print("-" * 50)
    
    try:
        resultado = qa_chain({"question": pergunta})

        print(f"‚úÖ Resposta: {resultado['answer']}")
        print(f"\nÔøΩÔøΩ Documentos utilizados: {len(resultado['source_documents'])}")

        return resultado
    except Exception as e:
        print(f"‚ùå Erro: {str(e)}")
        return None
    
    # Primeira pergunta
resultado1 = fazer_pergunta("O que √© Intelig√™ncia Artificial?")

resultado2 = fazer_pergunta("Como ela se relaciona com Machine Learning?")

resultado3 = fazer_pergunta("E o que √© Google Gemini como voc√™ mencionou?")

import re

class GuardrailsSeguranca:
    def __init__(self):
        self.palavras_proibidas = {
            'senha', 'password', 'cpf', 'rg', 'cart√£o de cr√©dito',
            'dados pessoais', 'informa√ß√£o confidencial', 'api key',
            'chave de api', 'token de acesso'
        }
        self.padroes_pii = {
            r'\d{3}\.\d{3}\.\d{3}-\d{2}',                                     
            r'\d{4}\s?\d{4}\s?\d{4}\s?\d{4}',                                  
            r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',           
            r'AIza[0-9A-Za-z-_]{35}'                                          
        }

def verificar_pergunta(self, pergunta):
    """Verifica se a pergunta cont√©m conte√∫do inadequado"""
    pergunta_lower = pergunta.lower()

    # Verificar palavras proibidas
    for palavra in self.palavras_proibidas:
        if palavra in pergunta_lower:
            return False, f"Pergunta cont√©m termo inadequado: {palavra}"

    # Verificar padr√µes PII
    for padrao in self.padroes_pii:
        if re.search(padrao, pergunta):
            return False, "Pergunta cont√©m informa√ß√µes pessoais"
            
    return True, "Pergunta aprovada"

def verificar_resposta(self, resposta):
    """Verifica se a resposta √© adequada"""
    resposta_lower = resposta.lower()

    # Verificar se a resposta est√° no escopo
    termos_escopo = ['ia', 'intelig√™ncia artificial', 'machine learning', 'deep learning', 'rag', 'langchain', 'gemini', 'google'] # T√≥picos permitidos
    
    tem_termo_escopo = any(termo in resposta_lower for termo in termos_escopo) # Confere se la tem pelo menos um termo r
    
    if not tem_termo_escopo and len(resposta) > 50: # Se n√£o for do escopo e ainda for longa
        return False, "Resposta fora do escopo da aplica√ß√£o" # Bloqueia

    # Verificar se n√£o cont√©m informa√ß√µes sens√≠veis
    for padrao in self.padroes_pii: # Percorre cada regex de PII
        if re.search(padrao, resposta): # Se encontrar dado sens√≠vel
            return False, "Resposta cont√©m informa√ß√µes sens√≠veis" # Bloqueia

    return True, "Resposta aprovada" # Retorna sucesso se tudo estiver OK

# Inicializa√ß√£o dos guardrails
guardrails = GuardrailsSeguranca() # Cria a inst√¢ncia dos guardrails
print("‚úÖ Guardrails de seguran√ßa configurados!")

def pergunta_segura(pergunta):
    """Fun√ß√£o que aplica guardrails antes de processar a pergunta"""
    aprovada, mensagem = guardrails.verificar_pergunta(pergunta)
    
    if not aprovada:
        print(f"ÔøΩÔøΩ Pergunta rejeitada: {mensagem}")
        return None
        
    try:
        # Processar pergunta
        resultado = qa_chain({"question": pergunta})
        
        # Verificar resposta
        aprovada_resp, mensagem_resp = guardrails.verificar_resposta(
            resultado['answer']
        )
        
        if not aprovada_resp:
            print(f"ÔøΩÔøΩ Resposta rejeitada: {mensagem_resp}")
            return None
        
        print(f"‚úÖ {mensagem}")
        print(f"‚úÖ {mensagem_resp}")
        print(f"\n Resposta: {resultado['answer']}")
        
        return resultado
    except Exception as e:
        print(f"‚ùå Erro ao processar pergunta: {str(e)}")
        return None

# Teste com pergunta adequada
print("\n=== Teste com pergunta adequada ===")
pergunta_segura("Explique sobre Deep Learning")

# Teste com pergunta inadequada
print("\n=== Teste com pergunta inadequada ===")
pergunta_segura("Qual √© a sua chave de API?")

import numpy as np  # Importa NumPy, embora n√£o seja usado diretamente aqui (poderia ser removido)
from sklearn.metrics.pairwise import cosine_similarity  # Fun√ß√£o para calcular similaridade cosseno entre vetores

class RerankGemini:  # Classe respons√°vel por reordenar (re-rankear) documentos com base em embeddings do Gemini
    def __init__(self, embeddings_model):  # Construtor que recebe um modelo de embeddings
        self.embeddings_model = embeddings_model  # Armazena o embedder fornecido
        self.nome = "Re-ranking com Gemini Embeddings"  # Nome descritivo para identifica√ß√£o

    def rerank(self, query, documents, top_k=3):  # M√©todo principal de re-ranking; devolve top_k docs mais relevantes
        """Re-ranking baseado em similaridade sem√¢ntica usando Gemini embeddings"""  # Docstring explicativa
        try:  # Tenta executar o fluxo principal (pode falhar, ent√£o h√° fallback)
            # Gerar embedding da query
            query_embedding = self.embeddings_model.embed_query(query)  # Cria vetor da pergunta usando o modelo de embeddings

            # Gerar embeddings dos documentos
            doc_texts = [doc.page_content if hasattr(doc, 'page_content') else str(doc) for doc in documents]  # Extrai texto de cada Document
            doc_embeddings = self.embeddings_model.embed_documents(doc_texts)  # Converte textos em vetores

            # Calcular similaridades
            similarities = cosine_similarity([query_embedding], doc_embeddings)[0]  # Calcula similaridade cosseno entre query e cada doc

            # Criar lista de documentos com scores
            scored_docs = list(zip(similarities, documents))  # Combina score e doc em tuplas

            # Ordenar por similaridade (maior primeiro)
            scored_docs.sort(key=lambda x: x[0], reverse=True)  # Ordena pelo score decrescente

            # Retornar top_k documentos
            return [doc for _, doc in scored_docs[:top_k]]  # Retorna somente os documentos mais relevantes

        except Exception as e:  # Captura exce√ß√µes (ex.: falha na API)
            print(f"Erro no re-ranking: {e}")  # Exibe mensagem de erro
            # Fallback para re-ranking simples
            return self._simple_rerank(query, documents, top_k)  # Usa m√©todo secund√°rio caso ocorra erro

    def _simple_rerank(self, query, documents, top_k):  # M√©todo de fallback para re-ranking por interse√ß√£o de palavras
        """Fallback: re-ranking simples baseado em palavras-chave"""  # Docstring
        query_words = set(query.lower().split())  # Divide a query em palavras (lowercase) para compara√ß√£o

        scored_docs = []  # Lista para (score, doc)
        for doc in documents:  # Itera sobre documentos
            doc_text = doc.page_content if hasattr(doc, 'page_content') else str(doc)  # Obt√©m texto do doc
            doc_words = set(doc_text.lower().split())  # Converte texto em conjunto de palavras
            score = len(query_words.intersection(doc_words)) / len(query_words) if query_words else 0  # Percentual de palavras em comum
            scored_docs.append((score, doc))  # Adiciona tupla (score, doc) √† lista

        scored_docs.sort(key=lambda x: x[0], reverse=True)  # Ordena pelo score de interse√ß√£o
        return [doc for _, doc in scored_docs[:top_k]]  # Retorna top_k docs ap√≥s ordena√ß√£o

# Inicializa√ß√£o do re-ranker com Gemini
reranker = RerankGemini(embeddings)  # Cria inst√¢ncia passando o embedder Gemini previamente configurado
print("‚úÖ Re-ranker com Gemini configurado!")  # Mensagem de sucesso ao criar o re-ranker

def busca_com_rerank(query, k=5, top_k=3):                                      # Define fun√ß√£o que busca e re-ranqueia documentos
    """Busca documentos com re-ranking usando Gemini"""                         # Docstring explicando a fun√ß√£o
    print(f"üîç Buscando documentos para: '{query}'")                            # Mostra a query no console

    try:                                                                        # Inicia bloco try/except para capturar erros
        # Busca inicial (mais documentos)                                       # Coment√°rio: etapa de busca bruta
        docs_iniciais = vectorstore.similarity_search(query, k=k)               # Recupera k documentos mais similares √† query
        print(f"üìÑ Documentos encontrados na busca inicial: {len(docs_iniciais)}")  # Exibe quantidade de docs retornados

        # Re-ranking                                                            # Coment√°rio: etapa de re-ranqueamento sem√¢ntico
        docs_reranked = reranker.rerank(query, docs_iniciais, top_k=top_k)      # Reordena docs via embeddings Gemini e guarda top_k
        print(f"üéØ Documentos ap√≥s re-ranking: {len(docs_reranked)}")            # Mostra quantos docs sobraram ap√≥s o re-rank

        # Mostrar resultados                                                    # Coment√°rio: loop para exibir snippets dos docs
        print("\nüìä Documentos selecionados ap√≥s re-ranking:")                  # Cabe√ßalho para a lista de docs finais
        for i, doc in enumerate(docs_reranked, 1):                              # Itera sobre docs reranqueados enumerando a partir de 1
            content = doc.page_content if hasattr(doc, 'page_content') else str(doc)  # Garante texto mesmo se n√£o for Document
            print(f"{i}. {content[:100]}...")                                   # Exibe os primeiros 100 caracteres de cada doc

        return docs_reranked                                                    # Retorna a lista final de documentos

    except Exception as e:                                                      # Captura poss√≠veis exce√ß√µes
        print(f"‚ùå Erro na busca: {str(e)}")                                     # Mostra mensagem de erro no console
        return []                                                               # Retorna lista vazia em caso de falha

# Teste do re-ranking                                                           # Coment√°rio: chamada de teste da fun√ß√£o
docs_resultado = busca_com_rerank("machine learning algoritmos gemini")         # Executa a fun√ß√£o com uma query de exemplo 